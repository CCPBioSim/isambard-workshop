{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising Model Parameters Using the Evolutionary Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import nglview as nv\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import isambard\n",
    "import isambard.optimisation.evo_optimizers as ev_opts\n",
    "from isambard.optimisation.evo_optimizers import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ball_and_stick(ampal):\n",
    "    view = nv.show_text(ampal.pdb)\n",
    "    view.add_ball_and_stick()\n",
    "    view.remove_cartoon()\n",
    "    return view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three things are required to run an optimisation:\n",
    "\n",
    "1. A specification\n",
    "1. A list of amino-acid sequences to be packed onto the model\n",
    "1. A list of parameters\n",
    "\n",
    "The first two are pretty straight forward, chances are you know the specification you want to use and the sequence you want to optimize. In this example I'll use the `CoiledCoil` specification, using the `from_parameters` class method and I'll use the sequence for the [basis-set dimer](http://www.rcsb.org/pdb/explore/explore.do?structureId=4DZM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specification = isambard.specifications.CoiledCoil.from_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ['EIAALKQEIAALKKENAALKWEIAALKQ', 'EIAALKQEIAALKKENAALKWEIAALKQ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so simple, but parameters require a bit more effort to set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the signature of `CoiledCoil.from_parameters` it looks like this:\n",
    "\n",
    "```\n",
    "from_parameters(\n",
    "    n,\n",
    "    aa=28,\n",
    "    major_radius=None,\n",
    "    major_pitch=None,\n",
    "    phi_c_alpha=26.42,\n",
    "    minor_helix_type='alpha',\n",
    "    auto_build=True\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is expecting arguments in this order, so we need to supply parameters to the optimizer that match this format. We can make a list of parameters using the `Parameter` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    Parameter.static('Oligomeric State', 2),\n",
    "    Parameter.static('Helix Length', 28),\n",
    "    Parameter.dynamic('Radius', 5.0, 1.0),\n",
    "    Parameter.dynamic('Pitch', 200, 60),\n",
    "    Parameter.dynamic('PhiCA', 283, 27),  # 283 is equivalent to a g position\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Note\n",
    "> \"Parameters\" in the `from_parameters` class method name is referring to the geometric parameters that describe a coiled-coil, not the `Parameter` class in the `evo_optimizer` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Parameter`s can be created using the `static` or `dynamic` class method. `Parameter`s created with `Parameter.static` will have fixed values during optimisation, whereas those created with `Parameter.dynamic` will be modified by the optimizer.\n",
    "\n",
    "The parameters are created using the following arguments:\n",
    "\n",
    "```python\n",
    "Parameter.static('Oligomeric State', 2)\n",
    "#                ^                   ^___ The static value\n",
    "#                |___ a human readable label\n",
    "\n",
    "Parameter.dynamic('Radius', 5.0, 1.0)\n",
    "#                 ^         ^    ^___ Value range\n",
    "#                 |         |___ Mean value\n",
    "#                 |___ a human readable label\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Both parameter types have a human-readable label. This is for your own reference really, you can use anything you like, as long as you know what parameter it corresponds to. Static parameters have a single value supplied, while the dynamic value has a mean value and a range. For example, in the radius parameter, we've given a mean value of `5.0`, with a range of `1.0`, this means that the optimizer can create values for radius of between `4.0` and `6.0`.\n",
    "\n",
    "Once you've made your parameter list you can test it to see if it can be used to generate a model correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_values = [x.default_value for x in parameters]\n",
    "print(default_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `default_value` property returns either the static value or the mean of a set of dynamic values. We can unpack this list in order to supply arguments to the specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = specification(*default_values)\n",
    "print(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_model.major_radii)\n",
    "print(test_model.major_pitches)\n",
    "print(test_model.phi_c_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model was created as expected using the values we supplied for the parameters. Now we're ready to run an optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Running an Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have all these things in place, you're ready to initialise the optimizer. There are 4 different evolutionary optimizers in the `evo_optimizers` module:\n",
    "\n",
    "1. `GA` (Genetic Algorithm) - Good at eliminating unfavourable regions of parameter space.\n",
    "1. `DE` (Differential Evolution) - A simple and versatile algorithm that quickly finds the minima in smooth energy landscapes.\n",
    "1. `PSO` - (Particle Swarm Optimisation) - A noisy algorithm that explores a lot of the parameter space.\n",
    "1. `CMAES` - (Covariance Matrix Adaptive Evolutionary Strategy) - Thoroughly explores regions of parameter space that are likely to contain the minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Note\n",
    "> It's difficult to know which algorithm is going to be best for your problem, so it's a good idea to try out all of them on a small scale. If you're really unsure about which one to use, the `GA` provides a good balance of parameter-space coverage and thoroughness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the signature for the `GA`:\n",
    "\n",
    "```python\n",
    "# init signature\n",
    "GA(specification, sequences, parameters, build_fn, eval_fn,\n",
    "   cxpb=0.5, mutpb=0.2, **kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that the `GA`, and in fact all of the evolutionary optimizers, have 5 required arguments to initialize them. We've already sorted out the first three (specification, sequence and parameters), what about `build_fn` and `eval_fn`? These arguments are functions that the optimizer will use to build a model from the parameters it generates and then to evaluate those models. This means that the optimizers are highly customisable (ask someone if you'd like to know more about this), but to get started, you might want to use the optimizer with predefined build and evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_ga = ev_opts.GA.buff_internal_eval(specification, sequences, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `buff_internal_eval` class method uses `ev_opts.default_build` as the build method and uses the BUFF internal energy as the evaluation function. **This is a good place to start, but this might not be appropriate for all problems.** BUFF is a very soft force field, which is good for parametric modelling as it tolerates small clashes that can occur as a result of using a geometric description with restricted degrees of freedom. For your problem you might need a harder force field, or even some completely different way of evaluating models, in which case you can use another evaluation function or make your own.\n",
    "\n",
    "Now we can start the optimizer using the `run_opt` method. `run_opt` has two required arguments: `pop_size` and `generations`. `pop_size` defines the number of children (i.e. sets of parameters) to be evaluated per generation, while `generations` defines the total number of generations you want to run. We can also supply an additional `cores` argument, which defines the number of CPU cores you wish to use for the optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running the next command might take a while! ~2 mins.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_ga.run_opt(50, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer prints various statistics about the optimisation both as it's running and after it finishes. After each generation is complete, a range of information about that generation is printed:\n",
    "\n",
    "* **gen** - the generation number\n",
    "* **evals** - the number of models built and evaluated in that generation. Even though a specific `pop_size` is specified, not all models are built, as models with the same or very similar parameters are not recreated. This is most obvious in the `GA` where parameters with a high \"fitness\" are retained in the subsequent generation.\n",
    "* **avg** - the average value of the evaluation metric for the generation.\n",
    "* **std** - the standard deviation of the evaluation metric for the generation.\n",
    "* **min** - the lowest (best) score in the generation.\n",
    "* **max** - the highest (worst) score in the generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Note\n",
    "> This logging information can be written to disk using the `log`, `log_path` and `run_id` keyword arguments on `run_opt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the optimisation has finished, the parameters and score for the best model are printed. You can get the AMPAL object for the best model by using the `best_model` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = opt_ga.best_model\n",
    "print(optimized_model)\n",
    "print(optimized_model.major_radii)\n",
    "print(optimized_model.major_pitches)\n",
    "print(optimized_model.phi_c_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then write the PDB to a file and take a look at your optimized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ball_and_stick(optimized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this was a very quick optimisation, and to find something closer to the global minima you may need larger values for `pop_size` and `generations`. The std column of output printed by the optimizer gives a good indication of the degree of convergence on a single solution during optimisation, but you should be critical of the optimisation, running it multiple times and evaluating the models produced thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating an Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some tools in the optimizer that can give information about the optimisation run that we have performed. The most useful of these is the `make_energy_funnel_data` function, which compares all of the models created during the optimisation to the best model produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funnel_data = opt_ga.make_energy_funnel_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(funnel_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`make_energy_funnel_data` returns a list of tuples containing the RMSD between a model and the best model from the optimisation, the BUFF internal energy for that model and the generation that the model came from. We can use these data to make a plot showing gross properties of the optimisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [x[0] for x in funnel_data]\n",
    "ys = [x[1] for x in funnel_data]\n",
    "zs = [x[2] for x in funnel_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(4, 4)\n",
    "ax.grid()\n",
    "ax.scatter(xs, ys, c=zs)\n",
    "ax.set_xlabel('RMSD (Å)')\n",
    "ax.set_ylabel('BUFF Internal Energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, every model produced by the optimizer is shown as a point, with its BUFF internal energy as the Y axis and the RMSD from the best scoring model as the X axis. The colour represents the generation, purple is the first generation and yellow is the last generation.\n",
    "\n",
    "You can see that there is some sort of structure to the data points, which makes sense; if models have similar RMSDs, they should probably have a similar BUFF internal energy.\n",
    "\n",
    "Let's take a closer look at the best scoring models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(4, 4)\n",
    "ax.grid()\n",
    "ax.scatter(xs, ys, c=zs)\n",
    "ax.set_ylim(top=-700)\n",
    "ax.set_xlabel('RMSD (Å)')\n",
    "ax.set_ylabel('BUFF Internal Energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can clearly see here that later generations have better scores, although this is specific to the `GA`, other optimisation algorithms may or may not do this depending on how they search the solution space.\n",
    "\n",
    "The most interesting thing about this plot is that you can see that there is a clear minimum, which indicates that this optimisation has converged well. However, there are a number of reasons you might not see this with your optimisation:\n",
    "\n",
    "1. You haven't sampled enough, increase the `pop_size` or number of `generations`\n",
    "1. Your sequence or parameterisation may not have a single deep minima, and might instead have a few similar minima or be quite smooth in general. This could mean that your sequence is quite plastic, allowing a large number of comformations to be adopted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Problem\n",
    "> Try running this optimisation using some of the other optimizer classes (`ev_opts.PSO.buff_internal_eval`, `ev_opts.DE.buff_internal_eval` or `ev_opts.CMAES.buff_internal_eval`). Does the energy funnel look different? Has the optimizer found the same solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evolutionary optimizers are very effective at searching parameter space, and can be used to fit parameters to a specific amino-acid sequence. Don't forget though that the optimizers are not magic bullets, and the output must be thoroughly scrutinized. If the optimization is not producing sensible results, try changing the evaluation function, it's easy to use your favorite all-atom force field, or even better, produce a meta-score combining a range of different metrics!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
